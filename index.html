<!DOCTYPE html>
<html>
<head lang="en">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

  <meta http-equiv="x-ua-compatible" content="ie=edge" />

  <title>NLMap-SayCan</title>

  <meta name="description" content="" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <!-- <base href="/"> -->

  <!--FACEBOOK-->
  <meta
    name="og:image"
    content="https://nlmap-saycan.github.io/figures/teaser.png"
  />
  <meta
    property="og:image"
    content="https://nlmap-saycan.github.io/figures/teaser.png"
  />
  <meta property="og:image:type" content="image/png" />
  <meta property="og:image:width" content="2000" />
  <meta property="og:image:height" content="900" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://nlmap-saycan.github.io/" />
  <meta property="og:title" content="NLMap-SayCan" />
  <meta
    property="og:description"
    content="Project page for Open-vocabulary Queryable Scene Representations for Real World Planning"
  />

  <!--TWITTER-->
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="NLMap-SayCan" />
  <meta
    name="twitter:description"
    content="Project page for Project page for Open-vocabulary Queryable Scene Representations for Real World Planning"
  />
  <meta
    name="twitter:image"
    content="https://nlmap-saycan.github.io/figures/teaser.png"
  />

  <!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
  <!-- Place favicon.ico in the root directory -->

  <link rel="stylesheet" href="./nl-map_files/bootstrap.min.css" />
  <link rel="stylesheet" href="./nl-map_files/font-awesome.min.css" />
  <link rel="stylesheet" href="./nl-map_files/codemirror.min.css" />
  <link rel="stylesheet" href="./nl-map_files/app.css" />

  <link rel="stylesheet" href="./nl-map_files/bootstrap.min(1).css" />

  <script src="./nl-map_files/jquery.min.js"></script>
  <script src="./nl-map_files/bootstrap.min.js"></script>
  <script src="./nl-map_files/codemirror.min.js"></script>
  <script src="./nl-map_files/clipboard.min.js"></script>

  <script src="./nl-map_files/app.js"></script>
</head>
<body>
    <div class="container" id="main">
      <div class="row">
        <h2 class="col-md-12 text-center">
          <br />Active Semantic Mapping for Household Robots:<br>Rapid Indoor Adaptation and Reduced User Burden<br />
        </h2>
      </div>
    </div>

    <div class="col-md-8 col-md-offset-2">
    <h3>NLMap + SayCan</h3>
    <p class="text-justify">
      NLMap can be combined with LLM planners to significantly augment the
      capability of real robot operation. We connect NLMap with SayCan, a
      recent work that uses LLM planners to let robots plan and execute
      according to natural language instructions. NLMap free SayCan from a
      fixed, hard-coded set of objects, locations, or executable options.
      SayCan also fails to plan with global context awareness which is
      addressed by NLMap. With NLMap, SayCan can now perform a great number
      of previously unachievable tasks.
      <br />
    </p>
    <img src="./figures/nlm_saycan.png" width="100%" />
    <p class="text-center">
      <small>
        <br />
        As shown in the diagram above, SayCan combines LLM scores and
        affordance to plan a sequence of steps to accomplish specified by
        instructions. SayCan relied on a fixed list of object names and
        locations so would fail on instructions like "Water the plant"
        beucase plant is not in its pre-defined list. NLMap, on the other
        hand, actively proposes involved objects, options and query the
        scene representation for object locations. It, therefore can perform
        the task "Water the plant".
      </small>
    </p>
  </div>
  
<!--     <h1 class="title">Active Semantic Mapping for Household Robots:<br>Rapid Indoor Adaptation and Reduced User Burden</h1> -->

    <p>Tomochika Ishikawa, Akira Taniguchi, Yoshinobu Hagiwara, Tadahiro Taniguchi</p>
    <a href="https://github.com/Tomochika-Ishikawa/Active-SpCoSLAM">Github</a>
    <h2 class="fixed-width">Abstract</h2>
    <p class="center-align fixed-width text-justify">Active semantic mapping is essential for service robots to quickly capture both the environment’s map and its spatial meaning, while also minimizing users’ burdens during robot operation and data collection. SpCoSLAM, which is a semantic mapping with place categorization and simultaneous localization and mapping (SLAM), offers the advantage of not being limited to predefined labels, making it well-suited for environmental adaptation. However, SpCoSLAM presents two issues that increase users’ burdens: 1) users struggle to efficiently determine a destination for the robot’s quick adaptation, and 2) providing instructions to the robot becomes repetitive and cumbersome. To address these challenges, we propose ActiveSpCoSLAM, which enables the robot to actively explore uncharted areas while employing CLIP as image captioning to provide a flexible vocabulary that replaces human instructions. The robot determines its actions by calculating information gain integrated from both semantics and SLAM uncertainties.</p>
    <img src="./images/graphical_model.png", style="width: 700px; height: auto;">

    <h2 class="fixed-width">Action determination</h2>
    <p class="center-align fixed-width">The robot navigates to the position with the maximum information gain (IG) among the candidate points. The IG in Active-SpCoSLAM consists of a weighted sum of three IGs. IGs in Active-SpCoSLAM consist of a weighted sum of three IGs: IGs related to location concepts, IGs related to self-location, and IGs related to maps, respectively.</p>
    <video src="./images/active_spcoslam_demo_IG.mp4", style="width: 700px; height: auto;" controls></video>

    <div class="col-md-8 col-md-offset-2">
        <h2 class="text-center">Learning spatial conept</h2>
        <p class="text-justify">
            First, the robot acquires multiple images of its surroundings with the head camera.
            Then, the images are input to ClipCap for caption generation, and the images are
            input to CNN to obtain image features. The robot learns location concepts by
            categorizing these, plus self-position, into three multimodal pieces of information.<\br>
        </p>
    </div>
    <script src="script.js"></script>
</body>
</html>
